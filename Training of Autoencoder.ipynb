{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f256ff60-3efb-4462-bcc0-8c4e098182c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba as nb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58fa9ee-4800-4509-8ad2-9aa5fb17a447",
   "metadata": {},
   "source": [
    "Training an Autoencoder\n",
    "---\n",
    "\n",
    "In this file I show how to build and train an autoencoder using my liblary $NeuralNet.py$ on example of load_digits dataset.    \n",
    "$NeuralNet.py$ is where I implemented a code that allows to create a fully connected Neural Networkhat, that \n",
    "can be optimzed using Stochastic Gradient Descent.   \n",
    "\n",
    "---\n",
    "\n",
    "1. Firstly I upload the activation functions and NN class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ece48a17-be8f-41ba-9e08-5b5f06cb5a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NeuralNet import leaky_relu\n",
    "from NeuralNet import sigmoid\n",
    "from NeuralNet import tanh\n",
    "from NeuralNet import NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7135f11e-6ab8-4115-a11f-92e1a2dda055",
   "metadata": {},
   "source": [
    "2. Then I upload and pre-process the data on which I will train my model on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d40b8d3-2320-4539-9afb-a1824e5a118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Load digits data\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target  # Needed for stratification\n",
    "\n",
    "# Normalize to [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Stratified split to preserve class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3d5396-65e6-4fe4-ac1e-741138e06109",
   "metadata": {},
   "source": [
    "3. Setting the Autoencoder's architecture and training it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d47df1a-9d26-4e41-9ecc-c3e4e874678b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500 - Loss: 5.38464\n",
      "Epoch 2/1500 - Loss: 3.48549\n",
      "Epoch 3/1500 - Loss: 3.32009\n",
      "Epoch 4/1500 - Loss: 3.29940\n",
      "Epoch 5/1500 - Loss: 3.28585\n",
      "Epoch 6/1500 - Loss: 3.26942\n",
      "Epoch 7/1500 - Loss: 3.13266\n",
      "Epoch 8/1500 - Loss: 3.03340\n",
      "Epoch 9/1500 - Loss: 3.01980\n",
      "Epoch 10/1500 - Loss: 2.97569\n",
      "Epoch 11/1500 - Loss: 2.88959\n",
      "Epoch 12/1500 - Loss: 2.85621\n",
      "Epoch 13/1500 - Loss: 2.82604\n",
      "Epoch 14/1500 - Loss: 2.79733\n",
      "Epoch 15/1500 - Loss: 2.77675\n",
      "Epoch 16/1500 - Loss: 2.75612\n",
      "Epoch 17/1500 - Loss: 2.73905\n",
      "Epoch 18/1500 - Loss: 2.72126\n",
      "Epoch 19/1500 - Loss: 2.68081\n",
      "Epoch 20/1500 - Loss: 2.18560\n",
      "Epoch 21/1500 - Loss: 2.17194\n",
      "Epoch 22/1500 - Loss: 2.15849\n",
      "Epoch 23/1500 - Loss: 2.14561\n",
      "Epoch 24/1500 - Loss: 2.13557\n",
      "Epoch 25/1500 - Loss: 2.12078\n",
      "Epoch 26/1500 - Loss: 2.11415\n",
      "Epoch 27/1500 - Loss: 2.10725\n",
      "Epoch 28/1500 - Loss: 2.09795\n",
      "Epoch 29/1500 - Loss: 2.08803\n",
      "Epoch 30/1500 - Loss: 2.08258\n",
      "Epoch 31/1500 - Loss: 2.06257\n",
      "Epoch 32/1500 - Loss: 2.04877\n",
      "Epoch 33/1500 - Loss: 2.03194\n",
      "Epoch 34/1500 - Loss: 2.01911\n",
      "Epoch 35/1500 - Loss: 2.02087\n",
      "Epoch 36/1500 - Loss: 1.99939\n",
      "Epoch 37/1500 - Loss: 1.98122\n",
      "Epoch 38/1500 - Loss: 1.96643\n",
      "Epoch 39/1500 - Loss: 1.93543\n",
      "Epoch 40/1500 - Loss: 1.90463\n",
      "Epoch 41/1500 - Loss: 1.88394\n",
      "Epoch 42/1500 - Loss: 1.87187\n",
      "Epoch 43/1500 - Loss: 1.86568\n",
      "Epoch 44/1500 - Loss: 1.85870\n",
      "Epoch 45/1500 - Loss: 1.85310\n",
      "Epoch 46/1500 - Loss: 1.84422\n",
      "Epoch 47/1500 - Loss: 1.84417\n",
      "Epoch 48/1500 - Loss: 1.83077\n",
      "Epoch 49/1500 - Loss: 1.82932\n",
      "Epoch 50/1500 - Loss: 1.82014\n",
      "Epoch 51/1500 - Loss: 1.82090\n",
      "Epoch 52/1500 - Loss: 1.80783\n",
      "Epoch 53/1500 - Loss: 1.79482\n",
      "Epoch 54/1500 - Loss: 1.78435\n",
      "Epoch 55/1500 - Loss: 1.76972\n",
      "Epoch 56/1500 - Loss: 1.76008\n",
      "Epoch 57/1500 - Loss: 1.75004\n",
      "Epoch 58/1500 - Loss: 1.73308\n",
      "Epoch 59/1500 - Loss: 1.72751\n",
      "Epoch 60/1500 - Loss: 1.71830\n",
      "Epoch 61/1500 - Loss: 1.70291\n",
      "Epoch 62/1500 - Loss: 1.69347\n",
      "Epoch 63/1500 - Loss: 1.68402\n",
      "Epoch 64/1500 - Loss: 1.67929\n",
      "Epoch 65/1500 - Loss: 1.67353\n",
      "Epoch 66/1500 - Loss: 1.67204\n",
      "Epoch 67/1500 - Loss: 1.66243\n",
      "Epoch 68/1500 - Loss: 1.65682\n",
      "Epoch 69/1500 - Loss: 1.65420\n",
      "Epoch 70/1500 - Loss: 1.65029\n",
      "Epoch 71/1500 - Loss: 1.64179\n",
      "Epoch 72/1500 - Loss: 1.63789\n",
      "Epoch 73/1500 - Loss: 1.63152\n",
      "Epoch 74/1500 - Loss: 1.63015\n",
      "Epoch 75/1500 - Loss: 1.63107\n",
      "Epoch 76/1500 - Loss: 1.62758\n",
      "Epoch 77/1500 - Loss: 1.62618\n",
      "Epoch 78/1500 - Loss: 1.62283\n",
      "Epoch 79/1500 - Loss: 1.61266\n",
      "Epoch 80/1500 - Loss: 1.61314\n",
      "Epoch 81/1500 - Loss: 1.60615\n",
      "Epoch 82/1500 - Loss: 1.54035\n",
      "Epoch 83/1500 - Loss: 1.48184\n",
      "Epoch 84/1500 - Loss: 1.46500\n",
      "Epoch 85/1500 - Loss: 1.45197\n",
      "Epoch 86/1500 - Loss: 1.43532\n",
      "Epoch 87/1500 - Loss: 1.43294\n",
      "Epoch 88/1500 - Loss: 1.44696\n",
      "Epoch 89/1500 - Loss: 1.44185\n",
      "Epoch 90/1500 - Loss: 1.42418\n",
      "Epoch 91/1500 - Loss: 1.41243\n",
      "Epoch 92/1500 - Loss: 1.40413\n",
      "Epoch 93/1500 - Loss: 1.39446\n",
      "Epoch 94/1500 - Loss: 1.39188\n",
      "Epoch 95/1500 - Loss: 1.39024\n",
      "Epoch 96/1500 - Loss: 1.38301\n",
      "Epoch 97/1500 - Loss: 1.37152\n",
      "Epoch 98/1500 - Loss: 1.37409\n",
      "Epoch 99/1500 - Loss: 1.36538\n",
      "Epoch 100/1500 - Loss: 1.36258\n",
      "Epoch 101/1500 - Loss: 1.35854\n",
      "Epoch 102/1500 - Loss: 1.35587\n",
      "Epoch 103/1500 - Loss: 1.35402\n",
      "Epoch 104/1500 - Loss: 1.34803\n",
      "Epoch 105/1500 - Loss: 1.33808\n",
      "Epoch 106/1500 - Loss: 1.33924\n",
      "Epoch 107/1500 - Loss: 1.32952\n",
      "Epoch 108/1500 - Loss: 1.32603\n",
      "Epoch 109/1500 - Loss: 1.32154\n",
      "Epoch 110/1500 - Loss: 1.31129\n",
      "Epoch 111/1500 - Loss: 1.30871\n",
      "Epoch 112/1500 - Loss: 1.29817\n",
      "Epoch 113/1500 - Loss: 1.29400\n",
      "Epoch 114/1500 - Loss: 1.28955\n",
      "Epoch 115/1500 - Loss: 1.28082\n",
      "Epoch 116/1500 - Loss: 1.27939\n",
      "Epoch 117/1500 - Loss: 1.27291\n",
      "Epoch 118/1500 - Loss: 1.27002\n",
      "Epoch 119/1500 - Loss: 1.27493\n",
      "Epoch 120/1500 - Loss: 1.26562\n",
      "Epoch 121/1500 - Loss: 1.26208\n",
      "Epoch 122/1500 - Loss: 1.25881\n",
      "Epoch 123/1500 - Loss: 1.25276\n",
      "Epoch 124/1500 - Loss: 1.25063\n",
      "Epoch 125/1500 - Loss: 1.25103\n",
      "Epoch 126/1500 - Loss: 1.25047\n",
      "Epoch 127/1500 - Loss: 1.24381\n",
      "Epoch 128/1500 - Loss: 1.23776\n",
      "Epoch 129/1500 - Loss: 1.24011\n",
      "Epoch 130/1500 - Loss: 1.23880\n",
      "Epoch 131/1500 - Loss: 1.23246\n",
      "Epoch 132/1500 - Loss: 1.23378\n",
      "Epoch 133/1500 - Loss: 1.22953\n",
      "Epoch 134/1500 - Loss: 1.23160\n",
      "Epoch 135/1500 - Loss: 1.22505\n",
      "Epoch 136/1500 - Loss: 1.22474\n",
      "Epoch 137/1500 - Loss: 1.22033\n",
      "Epoch 138/1500 - Loss: 1.21632\n",
      "Epoch 139/1500 - Loss: 1.21833\n",
      "Epoch 140/1500 - Loss: 1.22335\n",
      "Epoch 141/1500 - Loss: 1.21933\n",
      "Epoch 142/1500 - Loss: 1.20980\n",
      "Epoch 143/1500 - Loss: 1.20925\n",
      "Epoch 144/1500 - Loss: 1.21021\n",
      "Epoch 145/1500 - Loss: 1.20818\n",
      "Epoch 146/1500 - Loss: 1.20341\n",
      "Epoch 147/1500 - Loss: 1.19980\n",
      "Epoch 148/1500 - Loss: 1.20047\n",
      "Epoch 149/1500 - Loss: 1.19818\n",
      "Epoch 150/1500 - Loss: 1.20112\n",
      "Epoch 151/1500 - Loss: 1.20010\n",
      "Epoch 152/1500 - Loss: 1.19681\n",
      "Epoch 153/1500 - Loss: 1.19608\n",
      "Epoch 154/1500 - Loss: 1.20914\n",
      "Epoch 155/1500 - Loss: 1.19158\n",
      "Epoch 156/1500 - Loss: 1.18665\n",
      "Epoch 157/1500 - Loss: 1.18766\n",
      "Epoch 158/1500 - Loss: 1.18608\n",
      "Epoch 159/1500 - Loss: 1.18333\n",
      "Epoch 160/1500 - Loss: 1.18270\n",
      "Epoch 161/1500 - Loss: 1.18624\n",
      "Epoch 162/1500 - Loss: 1.18771\n",
      "Epoch 163/1500 - Loss: 1.19018\n",
      "Epoch 164/1500 - Loss: 1.18027\n",
      "Epoch 165/1500 - Loss: 1.17911\n",
      "Epoch 166/1500 - Loss: 1.17880\n",
      "Epoch 167/1500 - Loss: 1.17498\n",
      "Epoch 168/1500 - Loss: 1.17562\n",
      "Epoch 169/1500 - Loss: 1.18051\n",
      "Epoch 170/1500 - Loss: 1.17292\n",
      "Epoch 171/1500 - Loss: 1.17177\n",
      "Epoch 172/1500 - Loss: 1.17160\n",
      "Epoch 173/1500 - Loss: 1.16996\n",
      "Epoch 174/1500 - Loss: 1.16638\n",
      "Epoch 175/1500 - Loss: 1.16858\n",
      "Epoch 176/1500 - Loss: 1.16796\n",
      "Epoch 177/1500 - Loss: 1.16591\n",
      "Epoch 178/1500 - Loss: 1.16358\n",
      "Epoch 179/1500 - Loss: 1.16994\n",
      "Epoch 180/1500 - Loss: 1.16380\n",
      "Epoch 181/1500 - Loss: 1.15965\n",
      "Epoch 182/1500 - Loss: 1.15728\n",
      "Epoch 183/1500 - Loss: 1.15722\n",
      "Epoch 184/1500 - Loss: 1.15264\n",
      "Epoch 185/1500 - Loss: 1.15515\n",
      "Epoch 186/1500 - Loss: 1.15461\n",
      "Epoch 187/1500 - Loss: 1.14855\n",
      "Epoch 188/1500 - Loss: 1.15080\n",
      "Epoch 189/1500 - Loss: 1.15316\n",
      "Epoch 190/1500 - Loss: 1.14812\n",
      "Epoch 191/1500 - Loss: 1.14809\n",
      "Epoch 192/1500 - Loss: 1.14315\n",
      "Epoch 193/1500 - Loss: 1.14355\n",
      "Epoch 194/1500 - Loss: 1.14567\n",
      "Epoch 195/1500 - Loss: 1.14631\n",
      "Epoch 196/1500 - Loss: 1.14474\n",
      "Epoch 197/1500 - Loss: 1.14458\n",
      "Epoch 198/1500 - Loss: 1.14311\n",
      "Epoch 199/1500 - Loss: 1.13904\n",
      "Epoch 200/1500 - Loss: 1.14008\n",
      "Epoch 201/1500 - Loss: 1.13418\n",
      "Epoch 202/1500 - Loss: 1.13918\n",
      "Epoch 203/1500 - Loss: 1.13537\n",
      "Epoch 204/1500 - Loss: 1.13672\n",
      "Epoch 205/1500 - Loss: 1.13864\n",
      "Epoch 206/1500 - Loss: 1.14141\n",
      "Epoch 207/1500 - Loss: 1.13512\n",
      "Epoch 208/1500 - Loss: 1.13150\n",
      "Epoch 209/1500 - Loss: 1.13390\n",
      "Epoch 210/1500 - Loss: 1.13144\n",
      "Epoch 211/1500 - Loss: 1.12840\n",
      "Epoch 212/1500 - Loss: 1.12441\n",
      "Epoch 213/1500 - Loss: 1.12440\n",
      "Epoch 214/1500 - Loss: 1.12363\n",
      "Epoch 215/1500 - Loss: 1.12047\n",
      "Epoch 216/1500 - Loss: 1.12189\n",
      "Epoch 217/1500 - Loss: 1.11826\n",
      "Epoch 218/1500 - Loss: 1.11932\n",
      "Epoch 219/1500 - Loss: 1.12269\n",
      "Epoch 220/1500 - Loss: 1.12134\n",
      "Epoch 221/1500 - Loss: 1.11611\n",
      "Epoch 222/1500 - Loss: 1.11170\n",
      "Epoch 223/1500 - Loss: 1.11473\n",
      "Epoch 224/1500 - Loss: 1.12447\n",
      "Epoch 225/1500 - Loss: 1.11873\n",
      "Epoch 226/1500 - Loss: 1.11066\n",
      "Epoch 227/1500 - Loss: 1.10740\n",
      "Epoch 228/1500 - Loss: 1.10885\n",
      "Epoch 229/1500 - Loss: 1.11170\n",
      "Epoch 230/1500 - Loss: 1.10895\n",
      "Epoch 231/1500 - Loss: 1.10655\n",
      "Epoch 232/1500 - Loss: 1.10396\n",
      "Epoch 233/1500 - Loss: 1.10463\n",
      "Epoch 234/1500 - Loss: 1.10491\n",
      "Epoch 235/1500 - Loss: 1.10078\n",
      "Epoch 236/1500 - Loss: 1.10450\n",
      "Epoch 237/1500 - Loss: 1.09907\n",
      "Epoch 238/1500 - Loss: 1.10151\n",
      "Epoch 239/1500 - Loss: 1.09860\n",
      "Epoch 240/1500 - Loss: 1.10362\n",
      "Epoch 241/1500 - Loss: 1.10553\n",
      "Epoch 242/1500 - Loss: 1.09622\n",
      "Epoch 243/1500 - Loss: 1.09619\n",
      "Epoch 244/1500 - Loss: 1.09277\n",
      "Epoch 245/1500 - Loss: 1.09293\n",
      "Epoch 246/1500 - Loss: 1.09290\n",
      "Epoch 247/1500 - Loss: 1.09250\n",
      "Epoch 248/1500 - Loss: 1.08896\n",
      "Epoch 249/1500 - Loss: 1.09126\n",
      "Epoch 250/1500 - Loss: 1.09367\n",
      "Epoch 251/1500 - Loss: 1.11306\n",
      "Epoch 252/1500 - Loss: 1.09696\n",
      "Epoch 253/1500 - Loss: 1.09291\n",
      "Epoch 254/1500 - Loss: 1.08918\n",
      "Epoch 255/1500 - Loss: 1.08722\n",
      "Epoch 256/1500 - Loss: 1.08704\n",
      "Epoch 257/1500 - Loss: 1.08662\n",
      "Epoch 258/1500 - Loss: 1.08423\n",
      "Epoch 259/1500 - Loss: 1.08283\n",
      "Epoch 260/1500 - Loss: 1.08265\n",
      "Epoch 261/1500 - Loss: 1.07901\n",
      "Epoch 262/1500 - Loss: 1.07971\n",
      "Epoch 263/1500 - Loss: 1.07905\n",
      "Epoch 264/1500 - Loss: 1.09045\n",
      "Epoch 265/1500 - Loss: 1.08802\n",
      "Epoch 266/1500 - Loss: 1.07688\n",
      "Epoch 267/1500 - Loss: 1.07124\n",
      "Epoch 268/1500 - Loss: 1.07379\n",
      "Epoch 269/1500 - Loss: 1.07444\n",
      "Epoch 270/1500 - Loss: 1.07486\n",
      "Epoch 271/1500 - Loss: 1.07168\n",
      "Epoch 272/1500 - Loss: 1.07073\n",
      "Epoch 273/1500 - Loss: 1.07372\n",
      "Epoch 274/1500 - Loss: 1.10052\n",
      "Epoch 275/1500 - Loss: 1.08315\n",
      "Epoch 276/1500 - Loss: 1.07384\n",
      "Epoch 277/1500 - Loss: 1.07051\n",
      "Epoch 278/1500 - Loss: 1.06498\n",
      "Epoch 279/1500 - Loss: 1.06308\n",
      "Epoch 280/1500 - Loss: 1.06767\n",
      "Epoch 281/1500 - Loss: 1.06496\n",
      "Epoch 282/1500 - Loss: 1.06325\n",
      "Epoch 283/1500 - Loss: 1.06151\n",
      "Epoch 284/1500 - Loss: 1.06569\n",
      "Epoch 285/1500 - Loss: 1.07191\n",
      "Epoch 286/1500 - Loss: 1.06751\n",
      "Epoch 287/1500 - Loss: 1.07184\n",
      "Epoch 288/1500 - Loss: 1.07679\n",
      "Epoch 289/1500 - Loss: 1.07327\n",
      "Epoch 290/1500 - Loss: 1.06271\n",
      "Epoch 291/1500 - Loss: 1.06004\n",
      "Epoch 292/1500 - Loss: 1.06558\n",
      "Epoch 293/1500 - Loss: 1.06320\n",
      "Epoch 294/1500 - Loss: 1.06076\n",
      "Epoch 295/1500 - Loss: 1.06183\n",
      "Epoch 296/1500 - Loss: 1.05444\n",
      "Epoch 297/1500 - Loss: 1.05178\n",
      "Epoch 298/1500 - Loss: 1.05251\n",
      "Epoch 299/1500 - Loss: 1.05779\n",
      "Epoch 300/1500 - Loss: 1.06556\n",
      "Epoch 301/1500 - Loss: 1.06366\n",
      "Epoch 302/1500 - Loss: 1.05400\n",
      "Epoch 303/1500 - Loss: 1.04763\n",
      "Epoch 304/1500 - Loss: 1.04668\n",
      "Epoch 305/1500 - Loss: 1.04921\n",
      "Epoch 306/1500 - Loss: 1.04556\n",
      "Epoch 307/1500 - Loss: 1.04588\n",
      "Epoch 308/1500 - Loss: 1.04571\n",
      "Epoch 309/1500 - Loss: 1.04461\n",
      "Epoch 310/1500 - Loss: 1.04505\n",
      "Epoch 311/1500 - Loss: 1.05240\n",
      "Epoch 312/1500 - Loss: 1.04447\n",
      "Epoch 313/1500 - Loss: 1.04154\n",
      "Epoch 314/1500 - Loss: 1.04167\n",
      "Epoch 315/1500 - Loss: 1.04585\n",
      "Epoch 316/1500 - Loss: 1.04438\n",
      "Epoch 317/1500 - Loss: 1.03576\n",
      "Epoch 318/1500 - Loss: 1.03721\n",
      "Epoch 319/1500 - Loss: 1.03231\n",
      "Epoch 320/1500 - Loss: 1.03537\n",
      "Epoch 321/1500 - Loss: 1.03459\n",
      "Epoch 322/1500 - Loss: 1.03834\n",
      "Epoch 323/1500 - Loss: 1.03662\n",
      "Epoch 324/1500 - Loss: 1.02966\n",
      "Epoch 325/1500 - Loss: 1.03060\n",
      "Epoch 326/1500 - Loss: 1.03397\n",
      "Epoch 327/1500 - Loss: 1.02982\n",
      "Epoch 328/1500 - Loss: 1.02818\n",
      "Epoch 329/1500 - Loss: 1.02833\n",
      "Epoch 330/1500 - Loss: 1.02845\n",
      "Epoch 331/1500 - Loss: 1.02765\n",
      "Epoch 332/1500 - Loss: 1.02429\n",
      "Epoch 333/1500 - Loss: 1.02450\n",
      "Epoch 334/1500 - Loss: 1.02661\n",
      "Epoch 335/1500 - Loss: 1.02474\n",
      "Epoch 336/1500 - Loss: 1.02943\n",
      "Epoch 337/1500 - Loss: 1.02651\n",
      "Epoch 338/1500 - Loss: 1.02230\n",
      "Epoch 339/1500 - Loss: 1.02105\n",
      "Epoch 340/1500 - Loss: 1.02276\n",
      "Epoch 341/1500 - Loss: 1.01732\n",
      "Epoch 342/1500 - Loss: 1.01693\n",
      "Epoch 343/1500 - Loss: 1.02133\n",
      "Epoch 344/1500 - Loss: 1.02204\n",
      "Epoch 345/1500 - Loss: 1.02270\n",
      "Epoch 346/1500 - Loss: 1.01902\n",
      "Epoch 347/1500 - Loss: 1.01561\n",
      "Epoch 348/1500 - Loss: 1.02079\n",
      "Epoch 349/1500 - Loss: 1.01679\n",
      "Epoch 350/1500 - Loss: 1.02252\n",
      "Epoch 351/1500 - Loss: 1.01429\n",
      "Epoch 352/1500 - Loss: 1.01403\n",
      "Epoch 353/1500 - Loss: 1.01834\n",
      "Epoch 354/1500 - Loss: 1.01732\n",
      "Epoch 355/1500 - Loss: 1.01763\n",
      "Epoch 356/1500 - Loss: 1.01509\n",
      "Epoch 357/1500 - Loss: 1.01359\n",
      "Epoch 358/1500 - Loss: 1.01353\n",
      "Epoch 359/1500 - Loss: 1.01061\n",
      "Epoch 360/1500 - Loss: 1.00930\n",
      "Epoch 361/1500 - Loss: 1.00940\n",
      "Epoch 362/1500 - Loss: 1.01325\n",
      "Epoch 363/1500 - Loss: 1.00891\n",
      "Epoch 364/1500 - Loss: 1.00727\n",
      "Epoch 365/1500 - Loss: 1.00717\n",
      "Epoch 366/1500 - Loss: 1.00676\n",
      "Epoch 367/1500 - Loss: 1.00725\n",
      "Epoch 368/1500 - Loss: 1.00600\n",
      "Epoch 369/1500 - Loss: 1.00672\n",
      "Epoch 370/1500 - Loss: 1.00303\n",
      "Epoch 371/1500 - Loss: 1.01743\n",
      "Epoch 372/1500 - Loss: 1.01285\n",
      "Epoch 373/1500 - Loss: 1.00212\n",
      "Epoch 374/1500 - Loss: 0.99633\n",
      "Epoch 375/1500 - Loss: 1.00334\n",
      "Epoch 376/1500 - Loss: 1.00449\n",
      "Epoch 377/1500 - Loss: 1.00075\n",
      "Epoch 378/1500 - Loss: 0.99872\n",
      "Epoch 379/1500 - Loss: 1.00054\n",
      "Epoch 380/1500 - Loss: 0.99774\n",
      "Epoch 381/1500 - Loss: 0.99490\n",
      "Epoch 382/1500 - Loss: 1.00660\n",
      "Epoch 383/1500 - Loss: 1.00719\n",
      "Epoch 384/1500 - Loss: 1.00299\n",
      "Epoch 385/1500 - Loss: 1.00242\n",
      "Epoch 386/1500 - Loss: 0.99542\n",
      "Epoch 387/1500 - Loss: 0.99819\n",
      "Epoch 388/1500 - Loss: 0.99717\n",
      "Epoch 389/1500 - Loss: 0.99183\n",
      "Epoch 390/1500 - Loss: 0.99532\n",
      "Epoch 391/1500 - Loss: 0.99673\n",
      "Epoch 392/1500 - Loss: 0.99310\n",
      "Epoch 393/1500 - Loss: 0.99060\n",
      "Epoch 394/1500 - Loss: 0.99464\n",
      "Epoch 395/1500 - Loss: 0.99199\n",
      "Epoch 396/1500 - Loss: 0.99202\n",
      "Epoch 397/1500 - Loss: 0.98975\n",
      "Epoch 398/1500 - Loss: 0.99258\n",
      "Epoch 399/1500 - Loss: 0.99684\n",
      "Epoch 400/1500 - Loss: 0.99621\n",
      "Epoch 401/1500 - Loss: 0.98638\n",
      "Epoch 402/1500 - Loss: 0.98775\n",
      "Epoch 403/1500 - Loss: 0.98372\n",
      "Epoch 404/1500 - Loss: 0.98679\n",
      "Epoch 405/1500 - Loss: 0.98691\n",
      "Epoch 406/1500 - Loss: 0.98596\n",
      "Epoch 407/1500 - Loss: 0.98984\n",
      "Epoch 408/1500 - Loss: 0.98988\n",
      "Epoch 409/1500 - Loss: 0.98640\n",
      "Epoch 410/1500 - Loss: 0.99446\n",
      "Epoch 411/1500 - Loss: 0.98930\n",
      "Epoch 412/1500 - Loss: 0.99184\n",
      "Epoch 413/1500 - Loss: 0.98707\n",
      "Epoch 414/1500 - Loss: 0.98409\n",
      "Epoch 415/1500 - Loss: 0.98266\n",
      "Epoch 416/1500 - Loss: 0.98314\n",
      "Epoch 417/1500 - Loss: 0.98228\n",
      "Epoch 418/1500 - Loss: 0.98368\n",
      "Epoch 419/1500 - Loss: 0.98457\n",
      "Epoch 420/1500 - Loss: 0.99365\n",
      "Epoch 421/1500 - Loss: 0.98470\n",
      "Epoch 422/1500 - Loss: 0.98266\n",
      "Epoch 423/1500 - Loss: 0.98131\n",
      "Epoch 424/1500 - Loss: 0.97609\n",
      "Epoch 425/1500 - Loss: 0.97936\n",
      "Epoch 426/1500 - Loss: 0.97634\n",
      "Epoch 427/1500 - Loss: 0.98107\n",
      "Epoch 428/1500 - Loss: 0.98272\n",
      "Epoch 429/1500 - Loss: 0.98170\n",
      "Epoch 430/1500 - Loss: 0.98068\n",
      "Epoch 431/1500 - Loss: 0.97881\n",
      "Epoch 432/1500 - Loss: 0.98047\n",
      "Epoch 433/1500 - Loss: 0.97498\n",
      "Epoch 434/1500 - Loss: 0.98081\n",
      "Epoch 435/1500 - Loss: 0.97807\n",
      "Epoch 436/1500 - Loss: 0.98206\n",
      "Epoch 437/1500 - Loss: 0.97370\n",
      "Epoch 438/1500 - Loss: 0.97834\n",
      "Epoch 439/1500 - Loss: 0.98074\n",
      "Epoch 440/1500 - Loss: 0.97604\n",
      "Epoch 441/1500 - Loss: 0.98059\n",
      "Epoch 442/1500 - Loss: 0.97309\n",
      "Epoch 443/1500 - Loss: 0.97291\n",
      "Epoch 444/1500 - Loss: 0.97490\n",
      "Epoch 445/1500 - Loss: 0.97898\n",
      "Epoch 446/1500 - Loss: 0.97383\n",
      "Epoch 447/1500 - Loss: 0.97155\n",
      "Epoch 448/1500 - Loss: 0.96896\n",
      "Epoch 449/1500 - Loss: 0.96881\n",
      "Epoch 450/1500 - Loss: 0.97112\n",
      "Epoch 451/1500 - Loss: 0.97591\n",
      "Epoch 452/1500 - Loss: 0.96924\n",
      "Epoch 453/1500 - Loss: 0.97762\n",
      "Epoch 454/1500 - Loss: 0.97159\n",
      "Epoch 455/1500 - Loss: 0.97138\n",
      "Epoch 456/1500 - Loss: 0.97154\n",
      "Epoch 457/1500 - Loss: 0.97069\n",
      "Epoch 458/1500 - Loss: 0.97730\n",
      "Epoch 459/1500 - Loss: 0.97335\n",
      "Epoch 460/1500 - Loss: 0.97758\n",
      "Epoch 461/1500 - Loss: 0.97768\n",
      "Epoch 462/1500 - Loss: 0.96820\n",
      "Epoch 463/1500 - Loss: 0.96812\n",
      "Epoch 464/1500 - Loss: 0.97248\n",
      "Epoch 465/1500 - Loss: 0.96644\n",
      "Epoch 466/1500 - Loss: 0.97047\n",
      "Epoch 467/1500 - Loss: 0.96586\n",
      "Epoch 468/1500 - Loss: 0.96670\n",
      "Epoch 469/1500 - Loss: 0.96636\n",
      "Epoch 470/1500 - Loss: 0.96671\n",
      "Epoch 471/1500 - Loss: 0.96772\n",
      "Epoch 472/1500 - Loss: 0.97326\n",
      "Epoch 473/1500 - Loss: 0.96880\n",
      "Epoch 474/1500 - Loss: 0.97206\n",
      "Epoch 475/1500 - Loss: 0.96710\n",
      "Epoch 476/1500 - Loss: 0.96664\n",
      "Epoch 477/1500 - Loss: 0.96760\n",
      "Epoch 478/1500 - Loss: 0.96631\n",
      "Epoch 479/1500 - Loss: 0.96723\n",
      "Epoch 480/1500 - Loss: 0.96610\n",
      "Epoch 481/1500 - Loss: 0.96778\n",
      "Epoch 482/1500 - Loss: 0.96918\n",
      "Epoch 483/1500 - Loss: 0.96519\n",
      "Epoch 484/1500 - Loss: 0.96095\n",
      "Epoch 485/1500 - Loss: 0.96513\n",
      "Epoch 486/1500 - Loss: 0.96642\n",
      "Epoch 487/1500 - Loss: 0.97011\n",
      "Epoch 488/1500 - Loss: 0.96838\n",
      "Epoch 489/1500 - Loss: 0.97088\n",
      "Epoch 490/1500 - Loss: 0.97027\n",
      "Epoch 491/1500 - Loss: 0.96383\n",
      "Epoch 492/1500 - Loss: 0.96403\n",
      "Epoch 493/1500 - Loss: 0.97338\n",
      "Epoch 494/1500 - Loss: 0.96143\n",
      "Epoch 495/1500 - Loss: 0.96371\n",
      "Epoch 496/1500 - Loss: 0.96049\n",
      "Epoch 497/1500 - Loss: 0.95803\n",
      "Epoch 498/1500 - Loss: 0.95961\n",
      "Epoch 499/1500 - Loss: 0.96565\n",
      "Epoch 500/1500 - Loss: 0.96647\n",
      "Epoch 501/1500 - Loss: 0.95704\n",
      "Epoch 502/1500 - Loss: 0.95765\n",
      "Epoch 503/1500 - Loss: 0.95398\n",
      "Epoch 504/1500 - Loss: 0.95678\n",
      "Epoch 505/1500 - Loss: 0.95783\n",
      "Epoch 506/1500 - Loss: 0.95781\n",
      "Epoch 507/1500 - Loss: 0.95361\n",
      "Epoch 508/1500 - Loss: 0.95622\n",
      "Epoch 509/1500 - Loss: 0.95388\n",
      "Epoch 510/1500 - Loss: 0.95362\n",
      "Epoch 511/1500 - Loss: 0.95193\n",
      "Epoch 512/1500 - Loss: 0.95496\n",
      "Epoch 513/1500 - Loss: 0.95862\n",
      "Epoch 514/1500 - Loss: 0.96191\n",
      "Epoch 515/1500 - Loss: 0.95287\n",
      "Epoch 516/1500 - Loss: 0.95315\n",
      "Epoch 517/1500 - Loss: 0.95394\n",
      "Epoch 518/1500 - Loss: 0.95963\n",
      "Epoch 519/1500 - Loss: 0.95790\n",
      "Epoch 520/1500 - Loss: 0.95039\n",
      "Epoch 521/1500 - Loss: 0.94912\n",
      "Epoch 522/1500 - Loss: 0.94999\n",
      "Epoch 523/1500 - Loss: 0.95238\n",
      "Epoch 524/1500 - Loss: 0.95092\n",
      "Epoch 525/1500 - Loss: 0.95503\n",
      "Epoch 526/1500 - Loss: 0.95550\n",
      "Epoch 527/1500 - Loss: 0.95035\n",
      "Epoch 528/1500 - Loss: 0.95507\n",
      "Epoch 529/1500 - Loss: 0.94457\n",
      "Epoch 530/1500 - Loss: 0.94978\n",
      "Epoch 531/1500 - Loss: 0.95920\n",
      "Epoch 532/1500 - Loss: 0.95117\n",
      "Epoch 533/1500 - Loss: 0.94713\n",
      "Epoch 534/1500 - Loss: 0.95044\n",
      "Epoch 535/1500 - Loss: 0.95234\n",
      "Epoch 536/1500 - Loss: 0.95419\n",
      "Epoch 537/1500 - Loss: 0.95167\n",
      "Epoch 538/1500 - Loss: 0.95453\n",
      "Epoch 539/1500 - Loss: 0.94656\n",
      "Epoch 540/1500 - Loss: 0.95229\n",
      "Epoch 541/1500 - Loss: 0.94385\n",
      "Epoch 542/1500 - Loss: 0.94526\n",
      "Epoch 543/1500 - Loss: 0.94381\n",
      "Epoch 544/1500 - Loss: 0.94402\n",
      "Epoch 545/1500 - Loss: 0.94921\n",
      "Epoch 546/1500 - Loss: 0.94914\n",
      "Epoch 547/1500 - Loss: 0.94602\n",
      "Epoch 548/1500 - Loss: 0.94621\n",
      "Epoch 549/1500 - Loss: 0.94561\n",
      "Epoch 550/1500 - Loss: 0.94472\n",
      "Epoch 551/1500 - Loss: 0.94356\n",
      "Epoch 552/1500 - Loss: 0.94343\n",
      "Epoch 553/1500 - Loss: 0.94922\n",
      "Epoch 554/1500 - Loss: 0.98476\n",
      "Epoch 555/1500 - Loss: 0.97103\n",
      "Epoch 556/1500 - Loss: 0.94778\n",
      "Epoch 557/1500 - Loss: 0.95712\n",
      "Epoch 558/1500 - Loss: 0.94018\n",
      "Epoch 559/1500 - Loss: 0.93704\n",
      "Epoch 560/1500 - Loss: 0.94099\n",
      "Epoch 561/1500 - Loss: 0.93656\n",
      "Epoch 562/1500 - Loss: 0.93618\n",
      "Epoch 563/1500 - Loss: 0.93796\n",
      "Epoch 564/1500 - Loss: 0.94348\n",
      "Epoch 565/1500 - Loss: 0.94616\n",
      "Epoch 566/1500 - Loss: 0.94145\n",
      "Epoch 567/1500 - Loss: 0.94306\n",
      "Epoch 568/1500 - Loss: 0.94080\n",
      "Epoch 569/1500 - Loss: 0.94067\n",
      "Epoch 570/1500 - Loss: 0.94667\n",
      "Epoch 571/1500 - Loss: 0.94154\n",
      "Epoch 572/1500 - Loss: 0.95503\n",
      "Epoch 573/1500 - Loss: 0.93996\n",
      "Epoch 574/1500 - Loss: 0.93473\n",
      "Epoch 575/1500 - Loss: 0.93635\n",
      "Epoch 576/1500 - Loss: 0.93286\n",
      "Epoch 577/1500 - Loss: 0.94952\n",
      "Epoch 578/1500 - Loss: 0.93507\n",
      "Epoch 579/1500 - Loss: 0.93210\n",
      "Epoch 580/1500 - Loss: 0.93173\n",
      "Epoch 581/1500 - Loss: 0.94210\n",
      "Epoch 582/1500 - Loss: 0.93440\n",
      "Epoch 583/1500 - Loss: 0.92971\n",
      "Epoch 584/1500 - Loss: 0.93384\n",
      "Epoch 585/1500 - Loss: 0.93222\n",
      "Epoch 586/1500 - Loss: 0.93602\n",
      "Epoch 587/1500 - Loss: 0.93985\n",
      "Epoch 588/1500 - Loss: 0.93586\n",
      "Epoch 589/1500 - Loss: 0.93873\n",
      "Epoch 590/1500 - Loss: 0.93861\n",
      "Epoch 591/1500 - Loss: 0.95646\n",
      "Epoch 592/1500 - Loss: 0.95759\n",
      "Epoch 593/1500 - Loss: 0.94945\n",
      "Epoch 594/1500 - Loss: 0.93641\n",
      "Epoch 595/1500 - Loss: 0.93966\n",
      "Epoch 596/1500 - Loss: 0.93790\n",
      "Epoch 597/1500 - Loss: 0.93186\n",
      "Epoch 598/1500 - Loss: 0.93203\n",
      "Epoch 599/1500 - Loss: 0.93531\n",
      "Epoch 600/1500 - Loss: 0.93102\n",
      "Epoch 601/1500 - Loss: 0.92935\n",
      "Epoch 602/1500 - Loss: 0.92836\n",
      "Epoch 603/1500 - Loss: 0.93439\n",
      "Epoch 604/1500 - Loss: 0.93579\n",
      "Epoch 605/1500 - Loss: 0.93612\n",
      "Epoch 606/1500 - Loss: 0.93129\n",
      "Epoch 607/1500 - Loss: 0.92482\n",
      "Epoch 608/1500 - Loss: 0.93740\n",
      "Epoch 609/1500 - Loss: 0.93299\n",
      "Epoch 610/1500 - Loss: 0.92965\n",
      "Epoch 611/1500 - Loss: 0.94818\n",
      "Epoch 612/1500 - Loss: 0.95404\n",
      "Epoch 613/1500 - Loss: 0.95234\n",
      "Epoch 614/1500 - Loss: 0.93862\n",
      "Epoch 615/1500 - Loss: 0.92998\n",
      "Epoch 616/1500 - Loss: 0.93458\n",
      "Epoch 617/1500 - Loss: 0.92651\n",
      "Epoch 618/1500 - Loss: 0.94013\n",
      "Epoch 619/1500 - Loss: 0.93276\n",
      "Epoch 620/1500 - Loss: 0.92649\n",
      "Epoch 621/1500 - Loss: 0.93451\n",
      "Epoch 622/1500 - Loss: 0.92378\n",
      "Epoch 623/1500 - Loss: 0.92163\n",
      "Epoch 624/1500 - Loss: 0.92466\n",
      "Epoch 625/1500 - Loss: 0.92330\n",
      "Epoch 626/1500 - Loss: 0.92452\n",
      "Epoch 627/1500 - Loss: 0.91986\n",
      "Epoch 628/1500 - Loss: 0.92862\n",
      "Epoch 629/1500 - Loss: 0.92699\n",
      "Epoch 630/1500 - Loss: 0.93296\n",
      "Epoch 631/1500 - Loss: 0.93104\n",
      "Epoch 632/1500 - Loss: 0.93270\n",
      "Epoch 633/1500 - Loss: 0.92381\n",
      "Epoch 634/1500 - Loss: 0.92327\n",
      "Epoch 635/1500 - Loss: 0.92732\n",
      "Epoch 636/1500 - Loss: 0.92584\n",
      "Epoch 637/1500 - Loss: 0.92414\n",
      "Epoch 638/1500 - Loss: 0.92618\n",
      "Epoch 639/1500 - Loss: 0.92963\n",
      "Epoch 640/1500 - Loss: 0.92699\n",
      "Epoch 641/1500 - Loss: 0.92614\n",
      "Epoch 642/1500 - Loss: 0.92383\n",
      "Epoch 643/1500 - Loss: 0.93354\n",
      "Epoch 644/1500 - Loss: 0.94479\n",
      "Epoch 645/1500 - Loss: 0.93034\n",
      "Epoch 646/1500 - Loss: 0.92267\n",
      "Epoch 647/1500 - Loss: 0.92335\n",
      "Epoch 648/1500 - Loss: 0.92244\n",
      "Epoch 649/1500 - Loss: 0.93115\n",
      "Epoch 650/1500 - Loss: 0.92623\n",
      "Epoch 651/1500 - Loss: 0.92637\n",
      "Epoch 652/1500 - Loss: 0.92386\n",
      "Epoch 653/1500 - Loss: 0.92935\n",
      "Epoch 654/1500 - Loss: 0.96127\n",
      "Epoch 655/1500 - Loss: 0.94296\n",
      "Epoch 656/1500 - Loss: 0.92603\n",
      "Epoch 657/1500 - Loss: 0.92442\n",
      "Epoch 658/1500 - Loss: 0.92394\n",
      "Epoch 659/1500 - Loss: 0.92387\n",
      "Epoch 660/1500 - Loss: 0.92305\n",
      "Epoch 661/1500 - Loss: 0.91862\n",
      "Epoch 662/1500 - Loss: 0.91832\n",
      "Epoch 663/1500 - Loss: 0.91750\n",
      "Epoch 664/1500 - Loss: 0.91591\n",
      "Epoch 665/1500 - Loss: 0.91742\n",
      "Epoch 666/1500 - Loss: 0.91840\n",
      "Epoch 667/1500 - Loss: 0.91876\n",
      "Epoch 668/1500 - Loss: 0.92902\n",
      "Epoch 669/1500 - Loss: 0.91957\n",
      "Epoch 670/1500 - Loss: 0.93330\n",
      "Epoch 671/1500 - Loss: 0.93446\n",
      "Epoch 672/1500 - Loss: 0.91802\n",
      "Epoch 673/1500 - Loss: 0.91830\n",
      "Epoch 674/1500 - Loss: 0.91875\n",
      "Epoch 675/1500 - Loss: 0.92939\n",
      "Epoch 676/1500 - Loss: 0.92572\n",
      "Epoch 677/1500 - Loss: 0.91781\n",
      "Epoch 678/1500 - Loss: 0.91647\n",
      "Epoch 679/1500 - Loss: 0.91330\n",
      "Epoch 680/1500 - Loss: 0.91908\n",
      "Epoch 681/1500 - Loss: 0.91741\n",
      "Epoch 682/1500 - Loss: 0.91771\n",
      "Epoch 683/1500 - Loss: 0.92316\n",
      "Epoch 684/1500 - Loss: 0.91697\n",
      "Epoch 685/1500 - Loss: 0.91302\n",
      "Epoch 686/1500 - Loss: 0.91256\n",
      "Epoch 687/1500 - Loss: 0.92358\n",
      "Epoch 688/1500 - Loss: 0.92330\n",
      "Epoch 689/1500 - Loss: 0.91893\n",
      "Epoch 690/1500 - Loss: 0.91363\n",
      "Epoch 691/1500 - Loss: 0.91224\n",
      "Epoch 692/1500 - Loss: 0.91169\n",
      "Epoch 693/1500 - Loss: 0.91488\n",
      "Epoch 694/1500 - Loss: 0.91416\n",
      "Epoch 695/1500 - Loss: 0.91954\n",
      "Epoch 696/1500 - Loss: 0.92770\n",
      "Epoch 697/1500 - Loss: 0.91373\n",
      "Epoch 698/1500 - Loss: 0.91718\n",
      "Epoch 699/1500 - Loss: 0.92409\n",
      "Epoch 700/1500 - Loss: 0.92469\n",
      "Epoch 701/1500 - Loss: 0.91462\n",
      "Epoch 702/1500 - Loss: 0.91111\n",
      "Epoch 703/1500 - Loss: 0.91265\n",
      "Epoch 704/1500 - Loss: 0.92160\n",
      "Epoch 705/1500 - Loss: 0.91518\n",
      "Epoch 706/1500 - Loss: 0.91259\n",
      "Epoch 707/1500 - Loss: 0.91248\n",
      "Epoch 708/1500 - Loss: 0.93868\n",
      "Epoch 709/1500 - Loss: 0.91393\n",
      "Epoch 710/1500 - Loss: 0.90956\n",
      "Epoch 711/1500 - Loss: 0.90614\n",
      "Epoch 712/1500 - Loss: 0.92149\n",
      "Epoch 713/1500 - Loss: 0.94184\n",
      "Epoch 714/1500 - Loss: 0.92349\n",
      "Epoch 715/1500 - Loss: 0.91828\n",
      "Epoch 716/1500 - Loss: 0.91738\n",
      "Epoch 717/1500 - Loss: 0.91526\n",
      "Epoch 718/1500 - Loss: 0.91366\n",
      "Epoch 719/1500 - Loss: 0.91029\n",
      "Epoch 720/1500 - Loss: 0.91261\n",
      "Epoch 721/1500 - Loss: 0.91748\n",
      "Epoch 722/1500 - Loss: 0.91087\n",
      "Epoch 723/1500 - Loss: 0.92002\n",
      "Epoch 724/1500 - Loss: 0.90956\n",
      "Epoch 725/1500 - Loss: 0.91310\n",
      "Epoch 726/1500 - Loss: 0.91139\n",
      "Epoch 727/1500 - Loss: 0.90985\n",
      "Epoch 728/1500 - Loss: 0.91432\n",
      "Epoch 729/1500 - Loss: 0.91041\n",
      "Epoch 730/1500 - Loss: 0.91159\n",
      "Epoch 731/1500 - Loss: 0.93043\n",
      "Epoch 732/1500 - Loss: 0.91410\n",
      "Epoch 733/1500 - Loss: 0.90685\n",
      "Epoch 734/1500 - Loss: 0.91951\n",
      "Epoch 735/1500 - Loss: 0.92262\n",
      "Epoch 736/1500 - Loss: 0.91106\n",
      "Epoch 737/1500 - Loss: 0.90499\n",
      "Epoch 738/1500 - Loss: 0.90815\n",
      "Epoch 739/1500 - Loss: 0.90455\n",
      "Epoch 740/1500 - Loss: 0.90441\n",
      "Epoch 741/1500 - Loss: 0.91231\n",
      "Epoch 742/1500 - Loss: 0.92698\n",
      "Epoch 743/1500 - Loss: 0.92279\n",
      "Epoch 744/1500 - Loss: 0.91117\n",
      "Epoch 745/1500 - Loss: 0.91105\n",
      "Epoch 746/1500 - Loss: 0.90889\n",
      "Epoch 747/1500 - Loss: 0.90583\n",
      "Epoch 748/1500 - Loss: 0.90637\n",
      "Epoch 749/1500 - Loss: 0.90379\n",
      "Epoch 750/1500 - Loss: 0.91515\n",
      "Epoch 751/1500 - Loss: 0.90592\n",
      "Epoch 752/1500 - Loss: 0.90742\n",
      "Epoch 753/1500 - Loss: 0.90615\n",
      "Epoch 754/1500 - Loss: 0.91061\n",
      "Epoch 755/1500 - Loss: 0.90247\n",
      "Epoch 756/1500 - Loss: 0.90539\n",
      "Epoch 757/1500 - Loss: 0.91008\n",
      "Epoch 758/1500 - Loss: 0.91013\n",
      "Epoch 759/1500 - Loss: 0.90316\n",
      "Epoch 760/1500 - Loss: 0.90888\n",
      "Epoch 761/1500 - Loss: 0.92101\n",
      "Epoch 762/1500 - Loss: 0.91805\n",
      "Epoch 763/1500 - Loss: 0.93727\n",
      "Epoch 764/1500 - Loss: 0.91250\n",
      "Epoch 765/1500 - Loss: 0.91465\n",
      "Epoch 766/1500 - Loss: 0.90590\n",
      "Epoch 767/1500 - Loss: 0.90158\n",
      "Epoch 768/1500 - Loss: 0.90656\n",
      "Epoch 769/1500 - Loss: 0.91160\n",
      "Epoch 770/1500 - Loss: 0.90722\n",
      "Epoch 771/1500 - Loss: 0.91942\n",
      "Epoch 772/1500 - Loss: 0.92031\n",
      "Epoch 773/1500 - Loss: 0.91250\n",
      "Epoch 774/1500 - Loss: 0.90658\n",
      "Epoch 775/1500 - Loss: 0.90885\n",
      "Epoch 776/1500 - Loss: 0.92250\n",
      "Epoch 777/1500 - Loss: 0.91132\n",
      "Epoch 778/1500 - Loss: 0.89921\n",
      "Epoch 779/1500 - Loss: 0.90551\n",
      "Epoch 780/1500 - Loss: 0.93268\n",
      "Epoch 781/1500 - Loss: 0.92771\n",
      "Epoch 782/1500 - Loss: 0.91045\n",
      "Epoch 783/1500 - Loss: 0.90409\n",
      "Epoch 784/1500 - Loss: 0.90000\n",
      "Epoch 785/1500 - Loss: 0.90285\n",
      "Epoch 786/1500 - Loss: 0.92587\n",
      "Epoch 787/1500 - Loss: 0.90651\n",
      "Epoch 788/1500 - Loss: 0.90151\n",
      "Epoch 789/1500 - Loss: 0.90697\n",
      "Epoch 790/1500 - Loss: 0.89989\n",
      "Epoch 791/1500 - Loss: 0.90629\n",
      "Epoch 792/1500 - Loss: 0.90100\n",
      "Epoch 793/1500 - Loss: 0.90030\n",
      "Epoch 794/1500 - Loss: 0.90714\n",
      "Epoch 795/1500 - Loss: 0.90953\n",
      "Epoch 796/1500 - Loss: 0.89880\n",
      "Epoch 797/1500 - Loss: 0.90072\n",
      "Epoch 798/1500 - Loss: 0.90754\n",
      "Epoch 799/1500 - Loss: 0.90116\n",
      "Epoch 800/1500 - Loss: 0.89371\n",
      "Epoch 801/1500 - Loss: 0.90857\n",
      "Epoch 802/1500 - Loss: 0.90106\n",
      "Epoch 803/1500 - Loss: 0.90213\n",
      "Epoch 804/1500 - Loss: 0.89449\n",
      "Epoch 805/1500 - Loss: 0.89516\n",
      "Epoch 806/1500 - Loss: 0.90035\n",
      "Epoch 807/1500 - Loss: 0.90004\n",
      "Epoch 808/1500 - Loss: 0.89730\n",
      "Epoch 809/1500 - Loss: 0.89492\n",
      "Epoch 810/1500 - Loss: 0.89603\n",
      "Epoch 811/1500 - Loss: 0.88886\n",
      "Epoch 812/1500 - Loss: 0.89688\n",
      "Epoch 813/1500 - Loss: 0.89496\n",
      "Epoch 814/1500 - Loss: 0.89187\n",
      "Epoch 815/1500 - Loss: 0.89635\n",
      "Epoch 816/1500 - Loss: 0.89469\n",
      "Epoch 817/1500 - Loss: 0.89584\n",
      "Epoch 818/1500 - Loss: 0.89623\n",
      "Epoch 819/1500 - Loss: 0.89527\n",
      "Epoch 820/1500 - Loss: 0.89359\n",
      "Epoch 821/1500 - Loss: 0.89194\n",
      "Epoch 822/1500 - Loss: 0.89949\n",
      "Epoch 823/1500 - Loss: 0.91114\n",
      "Epoch 824/1500 - Loss: 0.90406\n",
      "Epoch 825/1500 - Loss: 0.89575\n",
      "Epoch 826/1500 - Loss: 0.92145\n",
      "Epoch 827/1500 - Loss: 0.89621\n",
      "Epoch 828/1500 - Loss: 0.89516\n",
      "Epoch 829/1500 - Loss: 0.90410\n",
      "Epoch 830/1500 - Loss: 0.91660\n",
      "Epoch 831/1500 - Loss: 0.89495\n",
      "Epoch 832/1500 - Loss: 0.89249\n",
      "Epoch 833/1500 - Loss: 0.90096\n",
      "Epoch 834/1500 - Loss: 0.89645\n",
      "Epoch 835/1500 - Loss: 0.88925\n",
      "Epoch 836/1500 - Loss: 0.89314\n",
      "Epoch 837/1500 - Loss: 0.90009\n",
      "Epoch 838/1500 - Loss: 0.89454\n",
      "Epoch 839/1500 - Loss: 0.89394\n",
      "Epoch 840/1500 - Loss: 0.89214\n",
      "Epoch 841/1500 - Loss: 0.89388\n",
      "Epoch 842/1500 - Loss: 0.90812\n",
      "Epoch 843/1500 - Loss: 0.88971\n",
      "Epoch 844/1500 - Loss: 0.89456\n",
      "Epoch 845/1500 - Loss: 0.89914\n",
      "Epoch 846/1500 - Loss: 0.88882\n",
      "Epoch 847/1500 - Loss: 0.89378\n",
      "Epoch 848/1500 - Loss: 0.88753\n",
      "Epoch 849/1500 - Loss: 0.88874\n",
      "Epoch 850/1500 - Loss: 0.89260\n",
      "Epoch 851/1500 - Loss: 0.90270\n",
      "Epoch 852/1500 - Loss: 0.89149\n",
      "Epoch 853/1500 - Loss: 0.88797\n",
      "Epoch 854/1500 - Loss: 0.88601\n",
      "Epoch 855/1500 - Loss: 0.88781\n",
      "Epoch 856/1500 - Loss: 0.89231\n",
      "Epoch 857/1500 - Loss: 0.89351\n",
      "Epoch 858/1500 - Loss: 0.88886\n",
      "Epoch 859/1500 - Loss: 0.89960\n",
      "Epoch 860/1500 - Loss: 0.89822\n",
      "Epoch 861/1500 - Loss: 0.89004\n",
      "Epoch 862/1500 - Loss: 0.88914\n",
      "Epoch 863/1500 - Loss: 0.88541\n",
      "Epoch 864/1500 - Loss: 0.89387\n",
      "Epoch 865/1500 - Loss: 0.89120\n",
      "Epoch 866/1500 - Loss: 0.89656\n",
      "Epoch 867/1500 - Loss: 0.88770\n",
      "Epoch 868/1500 - Loss: 0.89215\n",
      "Epoch 869/1500 - Loss: 0.88377\n",
      "Epoch 870/1500 - Loss: 0.89004\n",
      "Epoch 871/1500 - Loss: 0.88366\n",
      "Epoch 872/1500 - Loss: 0.88531\n",
      "Epoch 873/1500 - Loss: 0.89416\n",
      "Epoch 874/1500 - Loss: 0.89650\n",
      "Epoch 875/1500 - Loss: 0.88948\n",
      "Epoch 876/1500 - Loss: 0.89692\n",
      "Epoch 877/1500 - Loss: 0.90506\n",
      "Epoch 878/1500 - Loss: 0.89295\n",
      "Epoch 879/1500 - Loss: 0.88652\n",
      "Epoch 880/1500 - Loss: 0.88632\n",
      "Epoch 881/1500 - Loss: 0.88594\n",
      "Epoch 882/1500 - Loss: 0.88568\n",
      "Epoch 883/1500 - Loss: 0.88532\n",
      "Epoch 884/1500 - Loss: 0.88315\n",
      "Epoch 885/1500 - Loss: 0.88801\n",
      "Epoch 886/1500 - Loss: 0.88558\n",
      "Epoch 887/1500 - Loss: 0.89077\n",
      "Epoch 888/1500 - Loss: 0.89191\n",
      "Epoch 889/1500 - Loss: 0.89216\n",
      "Epoch 890/1500 - Loss: 0.89946\n",
      "Epoch 891/1500 - Loss: 0.88156\n",
      "Epoch 892/1500 - Loss: 0.88227\n",
      "Epoch 893/1500 - Loss: 0.87888\n",
      "Epoch 894/1500 - Loss: 0.87456\n",
      "Epoch 895/1500 - Loss: 0.87826\n",
      "Epoch 896/1500 - Loss: 0.89250\n",
      "Epoch 897/1500 - Loss: 0.89874\n",
      "Epoch 898/1500 - Loss: 0.88930\n",
      "Epoch 899/1500 - Loss: 0.88535\n",
      "Epoch 900/1500 - Loss: 0.88401\n",
      "Epoch 901/1500 - Loss: 0.88224\n",
      "Epoch 902/1500 - Loss: 0.88378\n",
      "Epoch 903/1500 - Loss: 0.87924\n",
      "Epoch 904/1500 - Loss: 0.88444\n",
      "Epoch 905/1500 - Loss: 0.88106\n",
      "Epoch 906/1500 - Loss: 0.88073\n",
      "Epoch 907/1500 - Loss: 0.89253\n",
      "Epoch 908/1500 - Loss: 0.88257\n",
      "Epoch 909/1500 - Loss: 0.88627\n",
      "Epoch 910/1500 - Loss: 0.89932\n",
      "Epoch 911/1500 - Loss: 0.88248\n",
      "Epoch 912/1500 - Loss: 0.88403\n",
      "Epoch 913/1500 - Loss: 0.87515\n",
      "Epoch 914/1500 - Loss: 0.87837\n",
      "Epoch 915/1500 - Loss: 0.87744\n",
      "Epoch 916/1500 - Loss: 0.94729\n",
      "Epoch 917/1500 - Loss: 0.95470\n",
      "Epoch 918/1500 - Loss: 0.90535\n",
      "Epoch 919/1500 - Loss: 0.90664\n",
      "Epoch 920/1500 - Loss: 0.89260\n",
      "Epoch 921/1500 - Loss: 0.89369\n",
      "Epoch 922/1500 - Loss: 0.90160\n",
      "Epoch 923/1500 - Loss: 0.88853\n",
      "Epoch 924/1500 - Loss: 0.91023\n",
      "Epoch 925/1500 - Loss: 0.88550\n",
      "Epoch 926/1500 - Loss: 0.87758\n",
      "Epoch 927/1500 - Loss: 0.87584\n",
      "Epoch 928/1500 - Loss: 0.87657\n",
      "Epoch 929/1500 - Loss: 0.87931\n",
      "Epoch 930/1500 - Loss: 0.87708\n",
      "Epoch 931/1500 - Loss: 0.87755\n",
      "Epoch 932/1500 - Loss: 0.88291\n",
      "Epoch 933/1500 - Loss: 0.87449\n",
      "Epoch 934/1500 - Loss: 0.87387\n",
      "Epoch 935/1500 - Loss: 0.87889\n",
      "Epoch 936/1500 - Loss: 0.87432\n",
      "Epoch 937/1500 - Loss: 0.87388\n",
      "Epoch 938/1500 - Loss: 0.87461\n",
      "Epoch 939/1500 - Loss: 0.87591\n",
      "Epoch 940/1500 - Loss: 0.89624\n",
      "Epoch 941/1500 - Loss: 0.88189\n",
      "Epoch 942/1500 - Loss: 0.88140\n",
      "Epoch 943/1500 - Loss: 0.88319\n",
      "Epoch 944/1500 - Loss: 0.87719\n",
      "Epoch 945/1500 - Loss: 0.87393\n",
      "Epoch 946/1500 - Loss: 0.87624\n",
      "Epoch 947/1500 - Loss: 0.87632\n",
      "Epoch 948/1500 - Loss: 0.87360\n",
      "Epoch 949/1500 - Loss: 0.88427\n",
      "Epoch 950/1500 - Loss: 0.87862\n",
      "Epoch 951/1500 - Loss: 0.87064\n",
      "Epoch 952/1500 - Loss: 0.88567\n",
      "Epoch 953/1500 - Loss: 0.96114\n",
      "Epoch 954/1500 - Loss: 0.88029\n",
      "Epoch 955/1500 - Loss: 0.87586\n",
      "Epoch 956/1500 - Loss: 0.87251\n",
      "Epoch 957/1500 - Loss: 0.87498\n",
      "Epoch 958/1500 - Loss: 0.87581\n",
      "Epoch 959/1500 - Loss: 0.87521\n",
      "Epoch 960/1500 - Loss: 0.87365\n",
      "Epoch 961/1500 - Loss: 0.88258\n",
      "Epoch 962/1500 - Loss: 0.87691\n",
      "Epoch 963/1500 - Loss: 0.86929\n",
      "Epoch 964/1500 - Loss: 0.86782\n",
      "Epoch 965/1500 - Loss: 0.86757\n",
      "Epoch 966/1500 - Loss: 0.86692\n",
      "Epoch 967/1500 - Loss: 0.86725\n",
      "Epoch 968/1500 - Loss: 0.86869\n",
      "Epoch 969/1500 - Loss: 0.86804\n",
      "Epoch 970/1500 - Loss: 0.87515\n",
      "Epoch 971/1500 - Loss: 0.87161\n",
      "Epoch 972/1500 - Loss: 0.86756\n",
      "Epoch 973/1500 - Loss: 0.86899\n",
      "Epoch 974/1500 - Loss: 0.87277\n",
      "Epoch 975/1500 - Loss: 0.86876\n",
      "Epoch 976/1500 - Loss: 0.87028\n",
      "Epoch 977/1500 - Loss: 0.86829\n",
      "Epoch 978/1500 - Loss: 0.87464\n",
      "Epoch 979/1500 - Loss: 0.88198\n",
      "Epoch 980/1500 - Loss: 0.87781\n",
      "Epoch 981/1500 - Loss: 0.86430\n",
      "Epoch 982/1500 - Loss: 0.86930\n",
      "Epoch 983/1500 - Loss: 0.87099\n",
      "Epoch 984/1500 - Loss: 0.86770\n",
      "Epoch 985/1500 - Loss: 0.86805\n",
      "Epoch 986/1500 - Loss: 0.87080\n",
      "Epoch 987/1500 - Loss: 0.85160\n",
      "Epoch 988/1500 - Loss: 0.81745\n",
      "Epoch 989/1500 - Loss: 0.80515\n",
      "Epoch 990/1500 - Loss: 0.80801\n",
      "Epoch 991/1500 - Loss: 0.79727\n",
      "Epoch 992/1500 - Loss: 0.78320\n",
      "Epoch 993/1500 - Loss: 0.77638\n",
      "Epoch 994/1500 - Loss: 0.77987\n",
      "Epoch 995/1500 - Loss: 0.77032\n",
      "Epoch 996/1500 - Loss: 0.76533\n",
      "Epoch 997/1500 - Loss: 0.76198\n",
      "Epoch 998/1500 - Loss: 0.76024\n",
      "Epoch 999/1500 - Loss: 0.76631\n",
      "Epoch 1000/1500 - Loss: 0.75877\n",
      "Epoch 1001/1500 - Loss: 0.75582\n",
      "Epoch 1002/1500 - Loss: 0.75675\n",
      "Epoch 1003/1500 - Loss: 0.75869\n",
      "Epoch 1004/1500 - Loss: 0.76284\n",
      "Epoch 1005/1500 - Loss: 0.75728\n",
      "Epoch 1006/1500 - Loss: 0.75476\n",
      "Epoch 1007/1500 - Loss: 0.75599\n",
      "Epoch 1008/1500 - Loss: 0.75765\n",
      "Epoch 1009/1500 - Loss: 0.76814\n",
      "Epoch 1010/1500 - Loss: 0.76713\n",
      "Epoch 1011/1500 - Loss: 0.75822\n",
      "Epoch 1012/1500 - Loss: 0.76495\n",
      "Epoch 1013/1500 - Loss: 0.76001\n",
      "Epoch 1014/1500 - Loss: 0.75658\n",
      "Epoch 1015/1500 - Loss: 0.75269\n",
      "Epoch 1016/1500 - Loss: 0.75553\n",
      "Epoch 1017/1500 - Loss: 0.78903\n",
      "Epoch 1018/1500 - Loss: 0.76041\n",
      "Epoch 1019/1500 - Loss: 0.76429\n",
      "Epoch 1020/1500 - Loss: 0.76143\n",
      "Epoch 1021/1500 - Loss: 0.75315\n",
      "Epoch 1022/1500 - Loss: 0.77297\n",
      "Epoch 1023/1500 - Loss: 0.75840\n",
      "Epoch 1024/1500 - Loss: 0.75474\n",
      "Epoch 1025/1500 - Loss: 0.75960\n",
      "Epoch 1026/1500 - Loss: 0.75277\n",
      "Epoch 1027/1500 - Loss: 0.75088\n",
      "Epoch 1028/1500 - Loss: 0.75441\n",
      "Epoch 1029/1500 - Loss: 0.76486\n",
      "Epoch 1030/1500 - Loss: 0.75638\n",
      "Epoch 1031/1500 - Loss: 0.75904\n",
      "Epoch 1032/1500 - Loss: 0.75769\n",
      "Epoch 1033/1500 - Loss: 0.75585\n",
      "Epoch 1034/1500 - Loss: 0.75452\n",
      "Epoch 1035/1500 - Loss: 0.74940\n",
      "Epoch 1036/1500 - Loss: 0.74794\n",
      "Epoch 1037/1500 - Loss: 0.74958\n",
      "Epoch 1038/1500 - Loss: 0.74170\n",
      "Epoch 1039/1500 - Loss: 0.75019\n",
      "Epoch 1040/1500 - Loss: 0.74805\n",
      "Epoch 1041/1500 - Loss: 0.75717\n",
      "Epoch 1042/1500 - Loss: 0.75299\n",
      "Epoch 1043/1500 - Loss: 0.75179\n",
      "Epoch 1044/1500 - Loss: 0.79707\n",
      "Epoch 1045/1500 - Loss: 0.76077\n",
      "Epoch 1046/1500 - Loss: 0.74885\n",
      "Epoch 1047/1500 - Loss: 0.74560\n",
      "Epoch 1048/1500 - Loss: 0.74495\n",
      "Epoch 1049/1500 - Loss: 0.74733\n",
      "Epoch 1050/1500 - Loss: 0.74603\n",
      "Epoch 1051/1500 - Loss: 0.74633\n",
      "Epoch 1052/1500 - Loss: 0.74420\n",
      "Epoch 1053/1500 - Loss: 0.74277\n",
      "Epoch 1054/1500 - Loss: 0.74165\n",
      "Epoch 1055/1500 - Loss: 0.74373\n",
      "Epoch 1056/1500 - Loss: 0.75033\n",
      "Epoch 1057/1500 - Loss: 0.75040\n",
      "Epoch 1058/1500 - Loss: 0.74547\n",
      "Epoch 1059/1500 - Loss: 0.74450\n",
      "Epoch 1060/1500 - Loss: 0.74343\n",
      "Epoch 1061/1500 - Loss: 0.74561\n",
      "Epoch 1062/1500 - Loss: 0.74738\n",
      "Epoch 1063/1500 - Loss: 0.74702\n",
      "Epoch 1064/1500 - Loss: 0.74627\n",
      "Epoch 1065/1500 - Loss: 0.74141\n",
      "Epoch 1066/1500 - Loss: 0.74391\n",
      "Epoch 1067/1500 - Loss: 0.73755\n",
      "Epoch 1068/1500 - Loss: 0.74208\n",
      "Epoch 1069/1500 - Loss: 0.74611\n",
      "Epoch 1070/1500 - Loss: 0.74193\n",
      "Epoch 1071/1500 - Loss: 0.74548\n",
      "Epoch 1072/1500 - Loss: 0.74647\n",
      "Epoch 1073/1500 - Loss: 0.74155\n",
      "Epoch 1074/1500 - Loss: 0.74950\n",
      "Epoch 1075/1500 - Loss: 0.73868\n",
      "Epoch 1076/1500 - Loss: 0.74834\n",
      "Epoch 1077/1500 - Loss: 0.87183\n",
      "Epoch 1078/1500 - Loss: 0.83458\n",
      "Epoch 1079/1500 - Loss: 0.77298\n",
      "Epoch 1080/1500 - Loss: 0.75774\n",
      "Epoch 1081/1500 - Loss: 0.75716\n",
      "Epoch 1082/1500 - Loss: 0.75863\n",
      "Epoch 1083/1500 - Loss: 0.75393\n",
      "Epoch 1084/1500 - Loss: 0.74874\n",
      "Epoch 1085/1500 - Loss: 0.74370\n",
      "Epoch 1086/1500 - Loss: 0.74265\n",
      "Epoch 1087/1500 - Loss: 0.75030\n",
      "Epoch 1088/1500 - Loss: 0.74452\n",
      "Epoch 1089/1500 - Loss: 0.74409\n",
      "Epoch 1090/1500 - Loss: 0.74445\n",
      "Epoch 1091/1500 - Loss: 0.74112\n",
      "Epoch 1092/1500 - Loss: 0.74245\n",
      "Epoch 1093/1500 - Loss: 0.74128\n",
      "Epoch 1094/1500 - Loss: 0.73707\n",
      "Epoch 1095/1500 - Loss: 0.74303\n",
      "Epoch 1096/1500 - Loss: 0.73863\n",
      "Epoch 1097/1500 - Loss: 0.74061\n",
      "Epoch 1098/1500 - Loss: 0.74006\n",
      "Epoch 1099/1500 - Loss: 0.73896\n",
      "Epoch 1100/1500 - Loss: 0.74047\n",
      "Epoch 1101/1500 - Loss: 0.74597\n",
      "Epoch 1102/1500 - Loss: 0.73975\n",
      "Epoch 1103/1500 - Loss: 0.74297\n",
      "Epoch 1104/1500 - Loss: 0.74084\n",
      "Epoch 1105/1500 - Loss: 0.74121\n",
      "Epoch 1106/1500 - Loss: 0.74021\n",
      "Epoch 1107/1500 - Loss: 0.74644\n",
      "Epoch 1108/1500 - Loss: 0.74474\n",
      "Epoch 1109/1500 - Loss: 0.74073\n",
      "Epoch 1110/1500 - Loss: 0.73895\n",
      "Epoch 1111/1500 - Loss: 0.73618\n",
      "Epoch 1112/1500 - Loss: 0.73700\n",
      "Epoch 1113/1500 - Loss: 0.74536\n",
      "Epoch 1114/1500 - Loss: 0.74154\n",
      "Epoch 1115/1500 - Loss: 0.73501\n",
      "Epoch 1116/1500 - Loss: 0.74222\n",
      "Epoch 1117/1500 - Loss: 0.74219\n",
      "Epoch 1118/1500 - Loss: 0.74394\n",
      "Epoch 1119/1500 - Loss: 0.73953\n",
      "Epoch 1120/1500 - Loss: 0.74545\n",
      "Epoch 1121/1500 - Loss: 0.74242\n",
      "Epoch 1122/1500 - Loss: 0.75458\n",
      "Epoch 1123/1500 - Loss: 0.81071\n",
      "Epoch 1124/1500 - Loss: 0.77264\n",
      "Epoch 1125/1500 - Loss: 0.77699\n",
      "Epoch 1126/1500 - Loss: 0.74415\n",
      "Epoch 1127/1500 - Loss: 0.74013\n",
      "Epoch 1128/1500 - Loss: 0.74457\n",
      "Epoch 1129/1500 - Loss: 0.73823\n",
      "Epoch 1130/1500 - Loss: 0.73460\n",
      "Epoch 1131/1500 - Loss: 0.73412\n",
      "Epoch 1132/1500 - Loss: 0.73889\n",
      "Epoch 1133/1500 - Loss: 0.73403\n",
      "Epoch 1134/1500 - Loss: 0.73186\n",
      "Epoch 1135/1500 - Loss: 0.73526\n",
      "Epoch 1136/1500 - Loss: 0.73865\n",
      "Epoch 1137/1500 - Loss: 0.73436\n",
      "Epoch 1138/1500 - Loss: 0.73830\n",
      "Epoch 1139/1500 - Loss: 0.74136\n",
      "Epoch 1140/1500 - Loss: 0.73463\n",
      "Epoch 1141/1500 - Loss: 0.73370\n",
      "Epoch 1142/1500 - Loss: 0.73448\n",
      "Epoch 1143/1500 - Loss: 0.73288\n",
      "Epoch 1144/1500 - Loss: 0.73056\n",
      "Epoch 1145/1500 - Loss: 0.73363\n",
      "Epoch 1146/1500 - Loss: 0.73460\n",
      "Epoch 1147/1500 - Loss: 0.73450\n",
      "Epoch 1148/1500 - Loss: 0.73469\n",
      "Epoch 1149/1500 - Loss: 0.73548\n",
      "Epoch 1150/1500 - Loss: 0.73376\n",
      "Epoch 1151/1500 - Loss: 0.73718\n",
      "Epoch 1152/1500 - Loss: 0.73398\n",
      "Epoch 1153/1500 - Loss: 0.73148\n",
      "Epoch 1154/1500 - Loss: 0.73247\n",
      "Epoch 1155/1500 - Loss: 0.73341\n",
      "Epoch 1156/1500 - Loss: 0.73829\n",
      "Epoch 1157/1500 - Loss: 0.73730\n",
      "Epoch 1158/1500 - Loss: 0.72974\n",
      "Epoch 1159/1500 - Loss: 0.73319\n",
      "Epoch 1160/1500 - Loss: 0.73487\n",
      "Epoch 1161/1500 - Loss: 0.73365\n",
      "Epoch 1162/1500 - Loss: 0.74693\n",
      "Epoch 1163/1500 - Loss: 0.73377\n",
      "Epoch 1164/1500 - Loss: 0.73824\n",
      "Epoch 1165/1500 - Loss: 0.72892\n",
      "Epoch 1166/1500 - Loss: 0.72715\n",
      "Epoch 1167/1500 - Loss: 0.73403\n",
      "Epoch 1168/1500 - Loss: 0.73180\n",
      "Epoch 1169/1500 - Loss: 0.73071\n",
      "Epoch 1170/1500 - Loss: 0.72837\n",
      "Epoch 1171/1500 - Loss: 0.73259\n",
      "Epoch 1172/1500 - Loss: 0.73621\n",
      "Epoch 1173/1500 - Loss: 0.74461\n",
      "Epoch 1174/1500 - Loss: 0.73396\n",
      "Epoch 1175/1500 - Loss: 0.73784\n",
      "Epoch 1176/1500 - Loss: 0.73411\n",
      "Epoch 1177/1500 - Loss: 0.73376\n",
      "Epoch 1178/1500 - Loss: 0.72892\n",
      "Epoch 1179/1500 - Loss: 0.74515\n",
      "Epoch 1180/1500 - Loss: 0.73065\n",
      "Epoch 1181/1500 - Loss: 0.73307\n",
      "Epoch 1182/1500 - Loss: 0.73325\n",
      "Epoch 1183/1500 - Loss: 0.72887\n",
      "Epoch 1184/1500 - Loss: 0.73082\n",
      "Epoch 1185/1500 - Loss: 0.72624\n",
      "Epoch 1186/1500 - Loss: 0.73088\n",
      "Epoch 1187/1500 - Loss: 0.72762\n",
      "Epoch 1188/1500 - Loss: 0.73213\n",
      "Epoch 1189/1500 - Loss: 0.72294\n",
      "Epoch 1190/1500 - Loss: 0.73616\n",
      "Epoch 1191/1500 - Loss: 0.72795\n",
      "Epoch 1192/1500 - Loss: 0.72814\n",
      "Epoch 1193/1500 - Loss: 0.72616\n",
      "Epoch 1194/1500 - Loss: 0.73164\n",
      "Epoch 1195/1500 - Loss: 0.73429\n",
      "Epoch 1196/1500 - Loss: 0.72974\n",
      "Epoch 1197/1500 - Loss: 0.73532\n",
      "Epoch 1198/1500 - Loss: 0.73066\n",
      "Epoch 1199/1500 - Loss: 0.73283\n",
      "Epoch 1200/1500 - Loss: 0.72316\n",
      "Epoch 1201/1500 - Loss: 0.72522\n",
      "Epoch 1202/1500 - Loss: 0.72606\n",
      "Epoch 1203/1500 - Loss: 0.72620\n",
      "Epoch 1204/1500 - Loss: 0.73263\n",
      "Epoch 1205/1500 - Loss: 0.72857\n",
      "Epoch 1206/1500 - Loss: 0.74075\n",
      "Epoch 1207/1500 - Loss: 0.73050\n",
      "Epoch 1208/1500 - Loss: 0.72481\n",
      "Epoch 1209/1500 - Loss: 0.72284\n",
      "Epoch 1210/1500 - Loss: 0.73347\n",
      "Epoch 1211/1500 - Loss: 0.72972\n",
      "Epoch 1212/1500 - Loss: 0.72986\n",
      "Epoch 1213/1500 - Loss: 0.72882\n",
      "Epoch 1214/1500 - Loss: 0.72584\n",
      "Epoch 1215/1500 - Loss: 0.77238\n",
      "Epoch 1216/1500 - Loss: 0.75504\n",
      "Epoch 1217/1500 - Loss: 0.73578\n",
      "Epoch 1218/1500 - Loss: 0.72948\n",
      "Epoch 1219/1500 - Loss: 0.72482\n",
      "Epoch 1220/1500 - Loss: 0.73347\n",
      "Epoch 1221/1500 - Loss: 0.72713\n",
      "Epoch 1222/1500 - Loss: 0.72572\n",
      "Epoch 1223/1500 - Loss: 0.72228\n",
      "Epoch 1224/1500 - Loss: 0.72662\n",
      "Epoch 1225/1500 - Loss: 0.72182\n",
      "Epoch 1226/1500 - Loss: 0.72090\n",
      "Epoch 1227/1500 - Loss: 0.72175\n",
      "Epoch 1228/1500 - Loss: 0.72609\n",
      "Epoch 1229/1500 - Loss: 0.74231\n",
      "Epoch 1230/1500 - Loss: 0.72951\n",
      "Epoch 1231/1500 - Loss: 0.72654\n",
      "Epoch 1232/1500 - Loss: 0.72120\n",
      "Epoch 1233/1500 - Loss: 0.72483\n",
      "Epoch 1234/1500 - Loss: 0.71936\n",
      "Epoch 1235/1500 - Loss: 0.72524\n",
      "Epoch 1236/1500 - Loss: 0.72830\n",
      "Epoch 1237/1500 - Loss: 0.72596\n",
      "Epoch 1238/1500 - Loss: 0.72021\n",
      "Epoch 1239/1500 - Loss: 0.72909\n",
      "Epoch 1240/1500 - Loss: 0.72924\n",
      "Epoch 1241/1500 - Loss: 0.72333\n",
      "Epoch 1242/1500 - Loss: 0.73113\n",
      "Epoch 1243/1500 - Loss: 0.72160\n",
      "Epoch 1244/1500 - Loss: 0.72359\n",
      "Epoch 1245/1500 - Loss: 0.72903\n",
      "Epoch 1246/1500 - Loss: 0.78135\n",
      "Epoch 1247/1500 - Loss: 0.72943\n",
      "Epoch 1248/1500 - Loss: 0.71974\n",
      "Epoch 1249/1500 - Loss: 0.72668\n",
      "Epoch 1250/1500 - Loss: 0.72062\n",
      "Epoch 1251/1500 - Loss: 0.72288\n",
      "Epoch 1252/1500 - Loss: 0.74422\n",
      "Epoch 1253/1500 - Loss: 0.73150\n",
      "Epoch 1254/1500 - Loss: 0.73280\n",
      "Epoch 1255/1500 - Loss: 0.72467\n",
      "Epoch 1256/1500 - Loss: 0.71861\n",
      "Epoch 1257/1500 - Loss: 0.72147\n",
      "Epoch 1258/1500 - Loss: 0.72233\n",
      "Epoch 1259/1500 - Loss: 0.72657\n",
      "Epoch 1260/1500 - Loss: 0.72189\n",
      "Epoch 1261/1500 - Loss: 0.72144\n",
      "Epoch 1262/1500 - Loss: 0.71853\n",
      "Epoch 1263/1500 - Loss: 0.71829\n",
      "Epoch 1264/1500 - Loss: 0.71898\n",
      "Epoch 1265/1500 - Loss: 0.71643\n",
      "Epoch 1266/1500 - Loss: 0.72223\n",
      "Epoch 1267/1500 - Loss: 0.72257\n",
      "Epoch 1268/1500 - Loss: 0.71448\n",
      "Epoch 1269/1500 - Loss: 0.73561\n",
      "Epoch 1270/1500 - Loss: 0.76676\n",
      "Epoch 1271/1500 - Loss: 0.80001\n",
      "Epoch 1272/1500 - Loss: 0.73347\n",
      "Epoch 1273/1500 - Loss: 0.71674\n",
      "Epoch 1274/1500 - Loss: 0.72119\n",
      "Epoch 1275/1500 - Loss: 0.71907\n",
      "Epoch 1276/1500 - Loss: 0.71845\n",
      "Epoch 1277/1500 - Loss: 0.72155\n",
      "Epoch 1278/1500 - Loss: 0.71941\n",
      "Epoch 1279/1500 - Loss: 0.71661\n",
      "Epoch 1280/1500 - Loss: 0.72293\n",
      "Epoch 1281/1500 - Loss: 0.71139\n",
      "Epoch 1282/1500 - Loss: 0.71308\n",
      "Epoch 1283/1500 - Loss: 0.70980\n",
      "Epoch 1284/1500 - Loss: 0.71962\n",
      "Epoch 1285/1500 - Loss: 0.71177\n",
      "Epoch 1286/1500 - Loss: 0.71139\n",
      "Epoch 1287/1500 - Loss: 0.71926\n",
      "Epoch 1288/1500 - Loss: 0.71643\n",
      "Epoch 1289/1500 - Loss: 0.71316\n",
      "Epoch 1290/1500 - Loss: 0.70952\n",
      "Epoch 1291/1500 - Loss: 0.71640\n",
      "Epoch 1292/1500 - Loss: 0.71008\n",
      "Epoch 1293/1500 - Loss: 0.71774\n",
      "Epoch 1294/1500 - Loss: 0.73176\n",
      "Epoch 1295/1500 - Loss: 0.72067\n",
      "Epoch 1296/1500 - Loss: 0.71740\n",
      "Epoch 1297/1500 - Loss: 0.71240\n",
      "Epoch 1298/1500 - Loss: 0.71362\n",
      "Epoch 1299/1500 - Loss: 0.72148\n",
      "Epoch 1300/1500 - Loss: 0.71526\n",
      "Epoch 1301/1500 - Loss: 0.71271\n",
      "Epoch 1302/1500 - Loss: 0.71438\n",
      "Epoch 1303/1500 - Loss: 0.71044\n",
      "Epoch 1304/1500 - Loss: 0.71200\n",
      "Epoch 1305/1500 - Loss: 0.71386\n",
      "Epoch 1306/1500 - Loss: 0.71196\n",
      "Epoch 1307/1500 - Loss: 0.72292\n",
      "Epoch 1308/1500 - Loss: 0.71139\n",
      "Epoch 1309/1500 - Loss: 0.71874\n",
      "Epoch 1310/1500 - Loss: 0.71605\n",
      "Epoch 1311/1500 - Loss: 0.71431\n",
      "Epoch 1312/1500 - Loss: 0.70881\n",
      "Epoch 1313/1500 - Loss: 0.70865\n",
      "Epoch 1314/1500 - Loss: 0.70800\n",
      "Epoch 1315/1500 - Loss: 0.71363\n",
      "Epoch 1316/1500 - Loss: 0.71202\n",
      "Epoch 1317/1500 - Loss: 0.71691\n",
      "Epoch 1318/1500 - Loss: 0.70886\n",
      "Epoch 1319/1500 - Loss: 0.70653\n",
      "Epoch 1320/1500 - Loss: 0.71163\n",
      "Epoch 1321/1500 - Loss: 0.71534\n",
      "Epoch 1322/1500 - Loss: 0.71258\n",
      "Epoch 1323/1500 - Loss: 0.70881\n",
      "Epoch 1324/1500 - Loss: 0.71030\n",
      "Epoch 1325/1500 - Loss: 0.70541\n",
      "Epoch 1326/1500 - Loss: 0.70977\n",
      "Epoch 1327/1500 - Loss: 0.70837\n",
      "Epoch 1328/1500 - Loss: 0.71401\n",
      "Epoch 1329/1500 - Loss: 0.71540\n",
      "Epoch 1330/1500 - Loss: 0.71539\n",
      "Epoch 1331/1500 - Loss: 0.73434\n",
      "Epoch 1332/1500 - Loss: 0.73361\n",
      "Epoch 1333/1500 - Loss: 0.80914\n",
      "Epoch 1334/1500 - Loss: 0.79120\n",
      "Epoch 1335/1500 - Loss: 0.75541\n",
      "Epoch 1336/1500 - Loss: 0.72085\n",
      "Epoch 1337/1500 - Loss: 0.71201\n",
      "Epoch 1338/1500 - Loss: 0.71398\n",
      "Epoch 1339/1500 - Loss: 0.71064\n",
      "Epoch 1340/1500 - Loss: 0.70525\n",
      "Epoch 1341/1500 - Loss: 0.70586\n",
      "Epoch 1342/1500 - Loss: 0.70354\n",
      "Epoch 1343/1500 - Loss: 0.70041\n",
      "Epoch 1344/1500 - Loss: 0.70457\n",
      "Epoch 1345/1500 - Loss: 0.70634\n",
      "Epoch 1346/1500 - Loss: 0.70213\n",
      "Epoch 1347/1500 - Loss: 0.70571\n",
      "Epoch 1348/1500 - Loss: 0.70317\n",
      "Epoch 1349/1500 - Loss: 0.70227\n",
      "Epoch 1350/1500 - Loss: 0.70702\n",
      "Epoch 1351/1500 - Loss: 0.70583\n",
      "Epoch 1352/1500 - Loss: 0.70923\n",
      "Epoch 1353/1500 - Loss: 0.70546\n",
      "Epoch 1354/1500 - Loss: 0.70628\n",
      "Epoch 1355/1500 - Loss: 0.71133\n",
      "Epoch 1356/1500 - Loss: 0.70291\n",
      "Epoch 1357/1500 - Loss: 0.70640\n",
      "Epoch 1358/1500 - Loss: 0.69985\n",
      "Epoch 1359/1500 - Loss: 0.70969\n",
      "Epoch 1360/1500 - Loss: 0.71068\n",
      "Epoch 1361/1500 - Loss: 0.71311\n",
      "Epoch 1362/1500 - Loss: 0.70641\n",
      "Epoch 1363/1500 - Loss: 0.70759\n",
      "Epoch 1364/1500 - Loss: 0.70044\n",
      "Epoch 1365/1500 - Loss: 0.69951\n",
      "Epoch 1366/1500 - Loss: 0.70206\n",
      "Epoch 1367/1500 - Loss: 0.69763\n",
      "Epoch 1368/1500 - Loss: 0.71171\n",
      "Epoch 1369/1500 - Loss: 0.70047\n",
      "Epoch 1370/1500 - Loss: 0.70451\n",
      "Epoch 1371/1500 - Loss: 0.70440\n",
      "Epoch 1372/1500 - Loss: 0.70376\n",
      "Epoch 1373/1500 - Loss: 0.69793\n",
      "Epoch 1374/1500 - Loss: 0.70899\n",
      "Epoch 1375/1500 - Loss: 0.70616\n",
      "Epoch 1376/1500 - Loss: 0.70504\n",
      "Epoch 1377/1500 - Loss: 0.70401\n",
      "Epoch 1378/1500 - Loss: 0.69983\n",
      "Epoch 1379/1500 - Loss: 0.70629\n",
      "Epoch 1380/1500 - Loss: 0.70307\n",
      "Epoch 1381/1500 - Loss: 0.70447\n",
      "Epoch 1382/1500 - Loss: 0.70065\n",
      "Epoch 1383/1500 - Loss: 0.70287\n",
      "Epoch 1384/1500 - Loss: 0.71781\n",
      "Epoch 1385/1500 - Loss: 0.71398\n",
      "Epoch 1386/1500 - Loss: 0.70834\n",
      "Epoch 1387/1500 - Loss: 0.71055\n",
      "Epoch 1388/1500 - Loss: 0.70589\n",
      "Epoch 1389/1500 - Loss: 0.72268\n",
      "Epoch 1390/1500 - Loss: 0.71122\n",
      "Epoch 1391/1500 - Loss: 0.70352\n",
      "Epoch 1392/1500 - Loss: 0.71351\n",
      "Epoch 1393/1500 - Loss: 0.70360\n",
      "Epoch 1394/1500 - Loss: 0.70263\n",
      "Epoch 1395/1500 - Loss: 0.70693\n",
      "Epoch 1396/1500 - Loss: 0.70585\n",
      "Epoch 1397/1500 - Loss: 0.69918\n",
      "Epoch 1398/1500 - Loss: 0.69530\n",
      "Epoch 1399/1500 - Loss: 0.70450\n",
      "Epoch 1400/1500 - Loss: 0.70557\n",
      "Epoch 1401/1500 - Loss: 0.69439\n",
      "Epoch 1402/1500 - Loss: 0.69833\n",
      "Epoch 1403/1500 - Loss: 0.69721\n",
      "Epoch 1404/1500 - Loss: 0.70118\n",
      "Epoch 1405/1500 - Loss: 0.69919\n",
      "Epoch 1406/1500 - Loss: 0.69469\n",
      "Epoch 1407/1500 - Loss: 0.70410\n",
      "Epoch 1408/1500 - Loss: 0.69770\n",
      "Epoch 1409/1500 - Loss: 0.70196\n",
      "Epoch 1410/1500 - Loss: 0.71327\n",
      "Epoch 1411/1500 - Loss: 0.70515\n",
      "Epoch 1412/1500 - Loss: 0.69656\n",
      "Epoch 1413/1500 - Loss: 0.71254\n",
      "Epoch 1414/1500 - Loss: 0.70506\n",
      "Epoch 1415/1500 - Loss: 0.69803\n",
      "Epoch 1416/1500 - Loss: 0.70121\n",
      "Epoch 1417/1500 - Loss: 0.70032\n",
      "Epoch 1418/1500 - Loss: 0.69203\n",
      "Epoch 1419/1500 - Loss: 0.69373\n",
      "Epoch 1420/1500 - Loss: 0.69442\n",
      "Epoch 1421/1500 - Loss: 0.70787\n",
      "Epoch 1422/1500 - Loss: 0.70805\n",
      "Epoch 1423/1500 - Loss: 0.71049\n",
      "Epoch 1424/1500 - Loss: 0.70298\n",
      "Epoch 1425/1500 - Loss: 0.70262\n",
      "Epoch 1426/1500 - Loss: 0.74660\n",
      "Epoch 1427/1500 - Loss: 0.72861\n",
      "Epoch 1428/1500 - Loss: 0.70796\n",
      "Epoch 1429/1500 - Loss: 0.71358\n",
      "Epoch 1430/1500 - Loss: 0.69479\n",
      "Epoch 1431/1500 - Loss: 0.69701\n",
      "Epoch 1432/1500 - Loss: 0.69593\n",
      "Epoch 1433/1500 - Loss: 0.69702\n",
      "Epoch 1434/1500 - Loss: 0.69932\n",
      "Epoch 1435/1500 - Loss: 0.68842\n",
      "Epoch 1436/1500 - Loss: 0.69009\n",
      "Epoch 1437/1500 - Loss: 0.69554\n",
      "Epoch 1438/1500 - Loss: 0.70622\n",
      "Epoch 1439/1500 - Loss: 0.70049\n",
      "Epoch 1440/1500 - Loss: 0.69952\n",
      "Epoch 1441/1500 - Loss: 0.70855\n",
      "Epoch 1442/1500 - Loss: 0.69322\n",
      "Epoch 1443/1500 - Loss: 0.69748\n",
      "Epoch 1444/1500 - Loss: 0.69660\n",
      "Epoch 1445/1500 - Loss: 0.69034\n",
      "Epoch 1446/1500 - Loss: 0.69528\n",
      "Epoch 1447/1500 - Loss: 0.69490\n",
      "Epoch 1448/1500 - Loss: 0.68985\n",
      "Epoch 1449/1500 - Loss: 0.69240\n",
      "Epoch 1450/1500 - Loss: 0.69388\n",
      "Epoch 1451/1500 - Loss: 0.69611\n",
      "Epoch 1452/1500 - Loss: 0.69636\n",
      "Epoch 1453/1500 - Loss: 0.68991\n",
      "Epoch 1454/1500 - Loss: 0.69103\n",
      "Epoch 1455/1500 - Loss: 0.69930\n",
      "Epoch 1456/1500 - Loss: 0.71209\n",
      "Epoch 1457/1500 - Loss: 0.71582\n",
      "Epoch 1458/1500 - Loss: 0.69699\n",
      "Epoch 1459/1500 - Loss: 0.70542\n",
      "Epoch 1460/1500 - Loss: 0.69486\n",
      "Epoch 1461/1500 - Loss: 0.70298\n",
      "Epoch 1462/1500 - Loss: 0.69839\n",
      "Epoch 1463/1500 - Loss: 0.69281\n",
      "Epoch 1464/1500 - Loss: 0.69356\n",
      "Epoch 1465/1500 - Loss: 0.69930\n",
      "Epoch 1466/1500 - Loss: 0.69316\n",
      "Epoch 1467/1500 - Loss: 0.69332\n",
      "Epoch 1468/1500 - Loss: 0.69530\n",
      "Epoch 1469/1500 - Loss: 0.69449\n",
      "Epoch 1470/1500 - Loss: 0.69252\n",
      "Epoch 1471/1500 - Loss: 0.69624\n",
      "Epoch 1472/1500 - Loss: 0.69781\n",
      "Epoch 1473/1500 - Loss: 0.69452\n",
      "Epoch 1474/1500 - Loss: 0.69361\n",
      "Epoch 1475/1500 - Loss: 0.71130\n",
      "Epoch 1476/1500 - Loss: 0.72619\n",
      "Epoch 1477/1500 - Loss: 0.69507\n",
      "Epoch 1478/1500 - Loss: 0.69575\n",
      "Epoch 1479/1500 - Loss: 0.69179\n",
      "Epoch 1480/1500 - Loss: 0.68944\n",
      "Epoch 1481/1500 - Loss: 0.74818\n",
      "Epoch 1482/1500 - Loss: 0.70306\n",
      "Epoch 1483/1500 - Loss: 0.69490\n",
      "Epoch 1484/1500 - Loss: 0.69246\n",
      "Epoch 1485/1500 - Loss: 0.69173\n",
      "Epoch 1486/1500 - Loss: 0.68388\n",
      "Epoch 1487/1500 - Loss: 0.68471\n",
      "Epoch 1488/1500 - Loss: 0.68698\n",
      "Epoch 1489/1500 - Loss: 0.68864\n",
      "Epoch 1490/1500 - Loss: 0.68934\n",
      "Epoch 1491/1500 - Loss: 0.68921\n",
      "Epoch 1492/1500 - Loss: 0.68990\n",
      "Epoch 1493/1500 - Loss: 0.69576\n",
      "Epoch 1494/1500 - Loss: 0.68394\n",
      "Epoch 1495/1500 - Loss: 0.68901\n",
      "Epoch 1496/1500 - Loss: 0.69241\n",
      "Epoch 1497/1500 - Loss: 0.69783\n",
      "Epoch 1498/1500 - Loss: 0.69456\n",
      "Epoch 1499/1500 - Loss: 0.69296\n",
      "Epoch 1500/1500 - Loss: 0.68785\n"
     ]
    }
   ],
   "source": [
    "# ========== Create the Autoencoder ==========\n",
    "autoencoder = NN(input_size=64)\n",
    "\n",
    "# -------- Encoder --------\n",
    "autoencoder.add_layer(50, activation_function=tanh)\n",
    "autoencoder.add_layer(40, activation_function=tanh)\n",
    "autoencoder.add_layer(36, activation_function=tanh)  # Bottleneck\n",
    "\n",
    "# -------- Decoder --------\n",
    "autoencoder.add_layer(40, activation_function=tanh)\n",
    "autoencoder.add_layer(50, activation_function=tanh)\n",
    "autoencoder.add_layer(64, activation_function=sigmoid)  # Output layer\n",
    "\n",
    "# ========== Training ==========\n",
    "n_epochs = 1500\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    losses = []\n",
    "    for x in X_train:\n",
    "        out = autoencoder._predict(x)\n",
    "        loss = autoencoder.backward(x, lr=learning_rate)  # x == y_true\n",
    "        losses.append(loss)\n",
    "\n",
    "    avg_loss = np.mean(losses)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} - Loss: {avg_loss:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6363f54-6704-4a5e-b9a7-ffc5b96cc132",
   "metadata": {},
   "source": [
    " Loss evaluation:\n",
    "--\n",
    "Loss for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecaf4005-db55-4a23-95a2-69b58f3e9e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Test Loss: 1.06882\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_losses = []\n",
    "for x in X_test:\n",
    "    out = autoencoder._predict(x)\n",
    "    loss = autoencoder.error(x)  # compare reconstructed x to original x\n",
    "    test_losses.append(loss)\n",
    "\n",
    "avg_test_loss = np.mean(test_losses)\n",
    "print(\"----------------------------------------\")\n",
    "print(f\"Test Loss: {avg_test_loss:.5f}\")\n",
    "print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37116e3-e1e6-4186-baa7-7bdfc2b40480",
   "metadata": {},
   "source": [
    "Here I show, how the model deals with example digid from the test set (it has not seen this image during training).\n",
    "\n",
    "Firstly, I show orginal image, then the compressed version (bottleneck) \n",
    "and at the end the reconstructed image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5cd4b48-cd83-4c32-86f3-e584db75b300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select a sample from the test set\n",
    "sample_idx = 4\n",
    "x_sample = X_test[sample_idx]\n",
    "\n",
    "# === Get original image ===\n",
    "original_image = x_sample.reshape(8, 8)\n",
    "\n",
    "# === Run through the autoencoder ===\n",
    "_ = autoencoder._predict(x_sample)  # Forward pass\n",
    "encoded = autoencoder.activations[3]  # Bottleneck output (32D)\n",
    "reconstructed = autoencoder.activations[-1].reshape(8, 8)\n",
    "\n",
    "# === Plotting ===\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# Original\n",
    "axs[0].imshow(original_image, cmap='binary')  # black digit, white background\n",
    "axs[0].set_title(\"Original Digit (64D)\")\n",
    "axs[0].axis('off')\n",
    "\n",
    "# Bottleneck\n",
    "axs[1].imshow(encoded.reshape(6, 6), cmap='binary')  # reshape to visualize\n",
    "axs[1].set_title(\"Bottleneck (36D)\")\n",
    "axs[1].axis('off')\n",
    "\n",
    "# Reconstructed\n",
    "axs[2].imshow(reconstructed, cmap='binary')\n",
    "axs[2].set_title(\"Reconstructed Digit (64D)\")\n",
    "axs[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286a955b-35e0-469b-90de-30c5d6f08cb9",
   "metadata": {},
   "source": [
    "Save the weights of the Autoencoder for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bdff125-e207-4857-bbbe-5058d60df1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get weights from each layer (list of lists of arrays)\n",
    "layer_weights = [layer.get_weights() for layer in autoencoder.layers]\n",
    "\n",
    "# Convert to numpy object array (this avoids the ValueError)\n",
    "layer_weights_obj = np.array(layer_weights, dtype=object)\n",
    "\n",
    "# Now save with pickle allowed\n",
    "np.save('layer_weights.npy', layer_weights_obj, allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3410c25c-5409-48cc-8903-c826a97bf827",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
